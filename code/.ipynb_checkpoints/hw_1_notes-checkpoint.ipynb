{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "import io\n",
    "from os import path\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "START = \"<START>\"\n",
    "UNK = \"<UNK>\"\n",
    "MAX_SIZE = 50\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    word_to_id = dict()\n",
    "    word_to_id['<PAD>'] = 0\n",
    "    word_to_id['<START>'] = 1\n",
    "    word_to_id['<UNK>'] = 2\n",
    "    index = 3\n",
    "    if isinstance(data, io.TextIOWrapper):\n",
    "        for line in data:\n",
    "            line = line.strip()\n",
    "            for i in range(len(line)):\n",
    "                if line[i] not in word_to_id:\n",
    "                    word_to_id[line[i]] = index\n",
    "                    index += 1\n",
    "                if i < len(line) - 2: \n",
    "                    if line[i:i+2] not in word_to_id:\n",
    "                        word_to_id[line[i:i+2]] = index\n",
    "                        index += 1\n",
    "    elif isinstance(data, str):\n",
    "        for i in range(len(data)):\n",
    "            if data[i] not in word_to_id:\n",
    "                word_to_id[data[i]] = index\n",
    "                index += 1\n",
    "            if i < len(data) - 2:\n",
    "                if data[i:i+2] not in word_to_id:\n",
    "                    word_to_id[data[i:i+2]] = index\n",
    "                    index += 1\n",
    "        \n",
    "    id_to_word = {v:k for k,v in word_to_id.items()}\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "with open(\"/Users/petercbu/Google Drive/University/2019 Spring/NLP/hw1_nlp_sapienza_2019/resources/train/input/as.txt\") as f:\n",
    "    word_to_id_as, id_to_word_as = build_vocab(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_id_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_dataset(file, word_to_id):\n",
    "    x = []\n",
    "    for line in file:\n",
    "        feature_vector = []\n",
    "        feature_vector.append(word_to_id[START])\n",
    "        \n",
    "        # Build feature vector\n",
    "        for i in range(len(line)):\n",
    "            unigram = line[i]\n",
    "            if unigram in word_to_id:\n",
    "                feature_vector.append(word_to_id[unigram])\n",
    "            else:\n",
    "                feature_vector.append(word_to_id[UNK])\n",
    "            \n",
    "            if i < len(line) - 2:\n",
    "                bigram = line[i:i+2]\n",
    "                if bigram in word_to_id:\n",
    "                    feature_vector.append(word_to_id[bigram])\n",
    "                else:\n",
    "                    feature_vector.append(word_to_id[UNK])\n",
    "        \n",
    "        x.append(np.array(feature_vector))\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIES_to_numerical(file_path):\n",
    "    BIES_to_number = {'B': 0, 'I': 1, 'E': 2, 'S': 3}\n",
    "    y = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            new_line = []\n",
    "            for ch in line:\n",
    "                new_line.append(str(BIES_to_number[ch]))\n",
    "            y.append(new_line)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/petercbu/Google Drive/University/2019 Spring/NLP/hw1_nlp_sapienza_2019/resources/train/input/as.txt\") as f:    \n",
    "    train_x_as = create_input_dataset(f, word_to_id_as)\n",
    "\n",
    "with open(\"/Users/petercbu/Google Drive/University/2019 Spring/NLP/hw1_nlp_sapienza_2019/resources/dev/input/as.txt\") as f:\n",
    "    dev_x_as = create_input_dataset(f, word_to_id_as)\n",
    "    \n",
    "train_y_as = BIES_to_numerical(\"/Users/petercbu/Google Drive/University/2019 Spring/NLP/hw1_nlp_sapienza_2019/resources/train/labels/as.txt\")\n",
    "dev_y_as = BIES_to_numerical(\"/Users/petercbu/Google Drive/University/2019 Spring/NLP/hw1_nlp_sapienza_2019/resources/dev/labels/as.txt\")\n",
    "\n",
    "train_x_as = pad_sequences(train_x_as, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_x_as = pad_sequences(dev_x_as, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "train_y_as = pad_sequences(train_y_as, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_y_as = pad_sequences(dev_y_as, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "\n",
    "#print(train_x_as[0:2])\n",
    "#print(train_y_as[0:2])\n",
    "#print(dev_x_as[0:2])\n",
    "#print(dev_y_as[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(708953, 50, 4)\n",
      "(14432, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "train_y_as = K.utils.to_categorical(train_y_as, 4, dtype='int')\n",
    "dev_y_as = K.utils.to_categorical(dev_y_as, 4, dtype='int')\n",
    "\n",
    "print(train_y_as.shape)\n",
    "print(dev_y_as.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size, dropout, recurrent_dropout):\n",
    "    print(\"Creating KERAS model\")\n",
    "\n",
    "    # define LSTM\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True)))\n",
    "    model.add(K.layers.Dense(4, activation='softmax'))\n",
    "    \n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 150)         105161850 \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 512)         833536    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 4)           2052      \n",
      "=================================================================\n",
      "Total params: 105,997,438\n",
      "Trainable params: 105,997,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting training...\n",
      "Train on 708953 samples, validate on 14432 samples\n",
      "Epoch 1/3\n",
      "  3296/708953 [..............................] - ETA: 18:47:47 - loss: 0.3358 - acc: 0.8487"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "EMBEDDING_SIZE = 150\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, 0.2, 0.2)\n",
    "# Let's print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")\n",
    "model.fit(train_x_as, train_y_as, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(dev_x_as, dev_y_as), callbacks=[cbk]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "#print(\"\\nEvaluating test...\")\n",
    "#loss_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

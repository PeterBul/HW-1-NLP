{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/DL/protobuf/lib/python3.6/site-packages/six-1.11.0-py3.6.egg (from keras) (1.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.14.5)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "import io\n",
    "from os import path\n",
    "from keras.layers import Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "MAX_SIZE = 24\n",
    "HIDDEN_SIZE = 256\n",
    "TRAIN_INPUT_PATH = \"../resources/train/input/msr.utf8\"\n",
    "DEV_INPUT_PATH = \"../resources/dev/input/msr.utf8\"\n",
    "TRAIN_LABELS_PATH = \"../resources/train/labels/msr.utf8\"\n",
    "DEV_LABELS_PATH = \"../resources/dev/labels/msr.utf8\"\n",
    "MODEL_WEIGHTS_PATH = \"my_model_weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    word_to_id_uni = dict()\n",
    "    word_to_id_bi = dict()\n",
    "    word_to_id_uni['<PAD>'] = 0\n",
    "    word_to_id_uni['<UNK>'] = 1\n",
    "    word_to_id_bi['<PAD>'] = 0\n",
    "    word_to_id_bi['<UNK>'] = 1\n",
    "    \n",
    "    uni_index = 2\n",
    "    bi_index = 2\n",
    "    \n",
    "    if isinstance(data, io.TextIOWrapper):\n",
    "        for line in data:\n",
    "            line = line.strip()\n",
    "            for i in range(len(line)):\n",
    "                if line[i] not in word_to_id_uni:\n",
    "                    word_to_id_uni[line[i]] = uni_index\n",
    "                    uni_index += 1\n",
    "                if i < len(line) - 2:\n",
    "                    if line[i:i + 2] not in word_to_id_bi:\n",
    "                        word_to_id_bi[line[i:i + 2]] = bi_index\n",
    "                        bi_index += 1\n",
    "\n",
    "\n",
    "    id_to_word_uni = {v: k for k, v in word_to_id_uni.items()}\n",
    "    id_to_word_bi = {v: k for k, v in word_to_id_bi.items()}\n",
    "\n",
    "    return word_to_id_uni, id_to_word_uni, word_to_id_bi, id_to_word_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size uni: 5169\n",
      "Vocab size bi: 424026\n"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "        TRAIN_INPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "    word_to_id_as_uni, id_to_word_as_uni, word_to_id_as_bi, id_to_word_as_bi = build_vocab(f)\n",
    "\n",
    "VOCAB_SIZE_UNI = len(word_to_id_as_uni)\n",
    "VOCAB_SIZE_BI = len(word_to_id_as_bi)\n",
    "\n",
    "print(\"Vocab size uni: \" + str(VOCAB_SIZE_UNI))\n",
    "print(\"Vocab size bi: \" + str(VOCAB_SIZE_BI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_dataset(file, word_to_id_uni, word_to_id_bi):\n",
    "    x_uni = []\n",
    "    x_bi = []\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        feature_vector_uni = []\n",
    "        feature_vector_bi = []\n",
    "        # Build feature vector\n",
    "        for i in range(len(line)):\n",
    "            unigram = line[i]\n",
    "            if unigram in word_to_id_uni:\n",
    "                feature_vector_uni.append(word_to_id_uni[unigram])\n",
    "            else:\n",
    "                feature_vector_uni.append(word_to_id_uni[UNK])\n",
    "\n",
    "            if i < len(line) - 2:\n",
    "                bigram = line[i:i + 2]\n",
    "                if bigram in word_to_id_bi:\n",
    "                    feature_vector_bi.append(word_to_id_bi[bigram])\n",
    "                else:\n",
    "                    feature_vector_bi.append(word_to_id_bi[UNK])\n",
    "\n",
    "        x_uni.append(np.array(feature_vector_uni))\n",
    "        x_bi.append(np.array(feature_vector_bi))\n",
    "    return np.array(x_uni), np.array(x_bi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIES_to_numerical(file_path):\n",
    "    BIES_to_number = {'B': 0, 'I': 1, 'E': 2, 'S': 3}\n",
    "    y = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            new_line = []\n",
    "            for ch in line:\n",
    "                new_line.append(str(BIES_to_number[ch]))\n",
    "            y.append(new_line)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        TRAIN_INPUT_PATH, 'r', encoding='utf8') as f:\n",
    "    train_x_as_uni, train_x_as_bi = create_input_dataset(f, word_to_id_as_uni, word_to_id_as_bi)\n",
    "\n",
    "with open(\n",
    "        DEV_INPUT_PATH, 'r', encoding='utf8') as f:\n",
    "    dev_x_as_uni, dev_x_as_bi = create_input_dataset(f, word_to_id_as_uni, word_to_id_as_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_as = BIES_to_numerical(TRAIN_LABELS_PATH)\n",
    "dev_y_as = BIES_to_numerical(DEV_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_as_uni = pad_sequences(train_x_as_uni, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "train_x_as_bi = pad_sequences(train_x_as_bi, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_x_as_uni = pad_sequences(dev_x_as_uni, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_x_as_bi = pad_sequences(dev_x_as_bi, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "train_y_as = pad_sequences(train_y_as, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_y_as = pad_sequences(dev_y_as, truncating='pre', padding='post', maxlen=MAX_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['可', '多', '得', '的', '教', '科', '书', '，', '她', '确', '实', '是', '名', '副', '其', '实', '的', '‘', '我', '的', '大', '学', '’', '。']\n",
      "\n",
      "['是不', '不可', '可多', '多得', '得的', '的教', '教科', '科书', '书，', '，她', '她确', '确实', '实是', '是名', '名副', '副其', '其实', '实的', '的‘', '‘我', '我的', '的大', '大学', '学’']\n",
      "[1 1 2 3 0 1 2 3 3 0 2 3 0 1 1 2 3 3 3 3 0 2 3 3]\n",
      "['扬', '帆', '远', '东', '做', '与', '中', '国', '合', '作', '的', '先', '行', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['扬帆', '帆远', '远东', '<UNK>', '做与', '与中', '中国', '国合', '合作', '作的', '的先', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "[0 2 0 2 3 3 0 2 0 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print([id_to_word_as_uni[i] for i in train_x_as_uni[0]])\n",
    "print()\n",
    "print([id_to_word_as_bi[i] for i in train_x_as_bi[0]])\n",
    "print(train_y_as[0])\n",
    "print([id_to_word_as_uni[i] for i in dev_x_as_uni[0]])\n",
    "print([id_to_word_as_bi[i] for i in dev_x_as_bi[0]])\n",
    "print(dev_y_as[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_as = K.utils.to_categorical(train_y_as, 4, dtype='int')\n",
    "dev_y_as = K.utils.to_categorical(dev_y_as, 4, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86924, 24)\n",
      "(86924, 24)\n",
      "(86924, 24, 4)\n",
      "(3985, 24)\n",
      "(3985, 24)\n",
      "(3985, 24, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_x_as_uni.shape)\n",
    "print(train_x_as_bi.shape)\n",
    "print(train_y_as.shape)\n",
    "print(dev_x_as_uni.shape)\n",
    "print(dev_x_as_bi.shape)\n",
    "print(dev_y_as.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['可', '多', '得', '的', '教', '科', '书', '，', '她', '确', '实', '是', '名', '副', '其', '实', '的', '‘', '我', '的', '大', '学', '’', '。']\n",
      "\n",
      "['是不', '不可', '可多', '多得', '得的', '的教', '教科', '科书', '书，', '，她', '她确', '确实', '实是', '是名', '名副', '副其', '其实', '实的', '的‘', '‘我', '我的', '的大', '大学', '学’']\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "[ 567 2444  560   55  162   18  504  327  679  183   20  113  344    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "[198681 175475 114528      1 269153  51534  14699 114417   4418    979\n",
      "   1505      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0]\n",
      "[[1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print([id_to_word_as_uni[i] for i in train_x_as_uni[0]])\n",
    "print()\n",
    "print([id_to_word_as_bi[i] for i in train_x_as_bi[0]])\n",
    "print(train_y_as[0])\n",
    "print(dev_x_as_uni[0])\n",
    "print(dev_x_as_bi[0])\n",
    "print(dev_y_as[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size_uni, vocab_size_bi, embedding_size, hidden_size, dropout, recurrent_dropout):\n",
    "    print(\"Creating KERAS model\")\n",
    "\n",
    "    # define LSTM\n",
    "    uni_input_layer = K.layers.Input((MAX_SIZE,))\n",
    "    bi_input_layer = K.layers.Input((MAX_SIZE,))\n",
    "    uni_embedding = K.layers.Embedding(vocab_size_uni, embedding_size, mask_zero=True)(uni_input_layer)\n",
    "    bi_embedding = K.layers.Embedding(vocab_size_bi, embedding_size, mask_zero=True)(bi_input_layer)\n",
    "    concatenated_layer = K.layers.concatenate([uni_embedding, bi_embedding], axis=-1)\n",
    "    bidirectional_layer = Bidirectional(LSTM(hidden_size, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True))(concatenated_layer)\n",
    "    output = K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax'))(bidirectional_layer)\n",
    "    \n",
    "    model = K.models.Model(inputs=[uni_input_layer, bi_input_layer], outputs=[output])\n",
    "\n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    optimizer = K.optimizers.Adam(lr=0.03)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 32)       165408      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 24, 32)       13568832    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 64)       0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 512)      657408      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 24, 4)        2052        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 14,393,700\n",
      "Trainable params: 14,393,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Starting training...\n",
      "Train on 86924 samples, validate on 3985 samples\n",
      "Epoch 1/50\n",
      "86924/86924 [==============================] - 458s 5ms/step - loss: 0.3833 - acc: 0.8258 - val_loss: 0.4688 - val_acc: 0.7789\n",
      "Epoch 2/50\n",
      "86924/86924 [==============================] - 456s 5ms/step - loss: 0.4933 - acc: 0.7660 - val_loss: 0.4761 - val_acc: 0.7737\n",
      "Epoch 3/50\n",
      "86924/86924 [==============================] - 455s 5ms/step - loss: 0.5004 - acc: 0.7613 - val_loss: 0.4852 - val_acc: 0.7702\n",
      "Epoch 4/50\n",
      "86924/86924 [==============================] - 458s 5ms/step - loss: 0.5041 - acc: 0.7593 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 5/50\n",
      "86924/86924 [==============================] - 459s 5ms/step - loss: 0.5055 - acc: 0.7583 - val_loss: 0.4864 - val_acc: 0.7682\n",
      "Epoch 6/50\n",
      "66368/86924 [=====================>........] - ETA: 1:46 - loss: 0.5064 - acc: 0.7576"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "EMBEDDING_SIZE = 32\n",
    "model = create_keras_model(VOCAB_SIZE_UNI, VOCAB_SIZE_BI, EMBEDDING_SIZE, HIDDEN_SIZE, 0.2, 0.2)\n",
    "# Let's print a summary of the model\n",
    "model.save_weights(MODEL_WEIGHTS_PATH)\n",
    "model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "model.summary()\n",
    "\n",
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")\n",
    "model.fit([train_x_as_uni, train_x_as_bi], train_y_as, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=([dev_x_as_uni, dev_x_as_bi], dev_y_as), callbacks=[cbk])\n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "#print(\"\\nEvaluating test...\")\n",
    "#loss_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/DL/protobuf/lib/python3.6/site-packages/six-1.11.0-py3.6.egg (from keras) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/anaconda3/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "import io\n",
    "from os import path\n",
    "from keras.layers import Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "MAX_SIZE = 24\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "\n",
    "def build_vocab(data):\n",
    "    word_to_id_uni = dict()\n",
    "    word_to_id_bi = dict()\n",
    "    word_to_id_uni['<PAD>'] = 0\n",
    "    word_to_id_uni['<UNK>'] = 1\n",
    "    word_to_id_bi['<PAD>'] = 0\n",
    "    word_to_id_bi['<UNK>'] = 1\n",
    "    \n",
    "    uni_index = 2\n",
    "    bi_index = 2\n",
    "    \n",
    "    if isinstance(data, io.TextIOWrapper):\n",
    "        for line in data:\n",
    "            line = line.strip()\n",
    "            for i in range(len(line)):\n",
    "                if line[i] not in word_to_id_uni:\n",
    "                    word_to_id_uni[line[i]] = uni_index\n",
    "                    uni_index += 1\n",
    "                if i < len(line) - 2:\n",
    "                    if line[i:i + 2] not in word_to_id_bi:\n",
    "                        word_to_id_bi[line[i:i + 2]] = bi_index\n",
    "                        bi_index += 1\n",
    "\n",
    "\n",
    "    id_to_word_uni = {v: k for k, v in word_to_id_uni.items()}\n",
    "    id_to_word_bi = {v: k for k, v in word_to_id_bi.items()}\n",
    "\n",
    "    return word_to_id_uni, id_to_word_uni, word_to_id_bi, id_to_word_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size uni: 5973\n",
      "Vocab size bi: 695107\n"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "        \"../resources/train/input/as.utf8\", 'r', encoding='utf-8') as f:\n",
    "    word_to_id_as_uni, id_to_word_as_uni, word_to_id_as_bi, id_to_word_as_bi = build_vocab(f)\n",
    "\n",
    "VOCAB_SIZE_UNI = len(word_to_id_as_uni)\n",
    "VOCAB_SIZE_BI = len(word_to_id_as_bi)\n",
    "\n",
    "print(\"Vocab size uni: \" + str(VOCAB_SIZE_UNI))\n",
    "print(\"Vocab size bi: \" + str(VOCAB_SIZE_BI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_dataset(file, word_to_id_uni, word_to_id_bi):\n",
    "    x_uni = []\n",
    "    x_bi = []\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        feature_vector_uni = []\n",
    "        feature_vector_bi = []\n",
    "        # Build feature vector\n",
    "        for i in range(len(line)):\n",
    "            unigram = line[i]\n",
    "            if unigram in word_to_id_uni:\n",
    "                feature_vector_uni.append(word_to_id_uni[unigram])\n",
    "            else:\n",
    "                feature_vector_uni.append(word_to_id_uni[UNK])\n",
    "\n",
    "            if i < len(line) - 2:\n",
    "                bigram = line[i:i + 2]\n",
    "                if bigram in word_to_id_bi:\n",
    "                    feature_vector_bi.append(word_to_id_bi[bigram])\n",
    "                else:\n",
    "                    feature_vector_bi.append(word_to_id_bi[UNK])\n",
    "\n",
    "        x_uni.append(np.array(feature_vector_uni))\n",
    "        x_bi.append(np.array(feature_vector_bi))\n",
    "    return np.array(x_uni), np.array(x_bi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIES_to_numerical(file_path):\n",
    "    BIES_to_number = {'B': 0, 'I': 1, 'E': 2, 'S': 3}\n",
    "    y = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            new_line = []\n",
    "            for ch in line:\n",
    "                new_line.append(str(BIES_to_number[ch]))\n",
    "            y.append(new_line)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        \"../resources/train/input/as.utf8\", 'r', encoding='utf8') as f:\n",
    "    train_x_as_uni, train_x_as_bi = create_input_dataset(f, word_to_id_as_uni, word_to_id_as_bi)\n",
    "\n",
    "with open(\n",
    "        \"../resources/dev/input/as.utf8\", 'r', encoding='utf8') as f:\n",
    "    dev_x_as_uni, dev_x_as_bi = create_input_dataset(f, word_to_id_as_uni, word_to_id_as_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_as = BIES_to_numerical(\n",
    "    \"../resources/train/labels/as.utf8\")\n",
    "dev_y_as = BIES_to_numerical(\n",
    "    \"../resources/dev/labels/as.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_as_uni = pad_sequences(train_x_as_uni, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "train_x_as_bi = pad_sequences(train_x_as_bi, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_x_as_uni = pad_sequences(dev_x_as_uni, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_x_as_bi = pad_sequences(dev_x_as_bi, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "train_y_as = pad_sequences(train_y_as, truncating='pre', padding='post', maxlen=MAX_SIZE)\n",
    "dev_y_as = pad_sequences(dev_y_as, truncating='pre', padding='post', maxlen=MAX_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['时', '间', '：', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "['时间', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "[0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 570  277   47  751   35 1520   19 2822  277  458  307  279  102  465\n",
      "   43 2991  793   43  774  404 1317 1400   67    0]\n",
      "[  1315  55623   2572 380895 115053 155731 110721      1 139791   8196\n",
      "  20771 104139  68517   4587 214844  66300  48607  10416  70196 225841\n",
      " 540274      0      0      0]\n",
      "[0 2 0 2 0 2 0 2 3 0 2 3 0 2 3 0 2 3 0 1 1 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "print([id_to_word_as_uni[i] for i in train_x_as_uni[0]])\n",
    "print()\n",
    "print([id_to_word_as_bi[i] for i in train_x_as_bi[0]])\n",
    "print(train_y_as[0])\n",
    "print(dev_x_as_uni[0])\n",
    "print(dev_x_as_bi[0])\n",
    "print(dev_y_as[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_as = K.utils.to_categorical(train_y_as, 4, dtype='int')\n",
    "dev_y_as = K.utils.to_categorical(dev_y_as, 4, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(708953, 24)\n",
      "(708953, 24)\n",
      "(708953, 24, 4)\n",
      "(14432, 24)\n",
      "(14432, 24)\n",
      "(14432, 24, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_x_as_uni.shape)\n",
    "print(train_x_as_bi.shape)\n",
    "print(train_y_as.shape)\n",
    "print(dev_x_as_uni.shape)\n",
    "print(dev_x_as_bi.shape)\n",
    "print(dev_y_as.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([id_to_word_as_uni[i] for i in train_x_as_uni[0]])\n",
    "#print()\n",
    "#print([id_to_word_as_bi[i] for i in train_x_as_bi[0]])\n",
    "#print(train_y_as[0])\n",
    "#print(dev_x_as_uni[0])\n",
    "#print(dev_x_as_bi[0])\n",
    "#print(dev_y_as[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size_uni, vocab_size_bi, embedding_size, hidden_size, dropout, recurrent_dropout):\n",
    "    print(\"Creating KERAS model\")\n",
    "\n",
    "    # define LSTM\n",
    "    uni_input_layer = K.layers.Input((MAX_SIZE,))\n",
    "    bi_input_layer = K.layers.Input((MAX_SIZE,))\n",
    "    uni_embedding = K.layers.Embedding(vocab_size_uni, embedding_size, mask_zero=True)(uni_input_layer)\n",
    "    bi_embedding = K.layers.Embedding(vocab_size_bi, embedding_size, mask_zero=True)(bi_input_layer)\n",
    "    concatenated_layer = K.layers.concatenate([uni_embedding, bi_embedding], axis=-1)\n",
    "    bidirectional_layer = Bidirectional(LSTM(hidden_size, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True))(concatenated_layer)\n",
    "    output = K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax'))(bidirectional_layer)\n",
    "    \n",
    "    model = K.models.Model(inputs=[uni_input_layer, bi_input_layer], outputs=[output])\n",
    "\n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 32)       191136      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 24, 32)       22243424    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 64)       0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 512)      657408      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 24, 4)        2052        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 23,094,020\n",
      "Trainable params: 23,094,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Starting training...\n",
      "Train on 708953 samples, validate on 14432 samples\n",
      "Epoch 1/3\n",
      " 41184/708953 [>.............................] - ETA: 1:00:45 - loss: 0.1920 - acc: 0.9212"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "EMBEDDING_SIZE = 32\n",
    "model = create_keras_model(VOCAB_SIZE_UNI, VOCAB_SIZE_BI, EMBEDDING_SIZE, HIDDEN_SIZE, 0.2, 0.2)\n",
    "# Let's print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")\n",
    "model.fit([train_x_as_uni, train_x_as_bi], train_y_as, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=([dev_x_as_uni, dev_x_as_bi], dev_y_as), callbacks=[cbk])\n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "#print(\"\\nEvaluating test...\")\n",
    "#loss_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
